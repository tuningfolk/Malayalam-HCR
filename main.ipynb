{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a712b5f9",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edd6472d-c004-44f0-92df-35f174c2fe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Malayalam letters mapping.\n",
    "import importlib\n",
    "import encoding\n",
    "importlib.reload(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52b63be",
   "metadata": {},
   "source": [
    "### Input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c527998-531e-4584-915c-cceeed0708c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'Sample-handwritten-text-input-for-OCR.png'\n",
    "image_path = 'mal-hw.jpeg'\n",
    "inp_img = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdb81ee",
   "metadata": {},
   "source": [
    "### Function declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "947ddf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_thresholding(image: Image, block_size=3, offset=0):\n",
    "    '''\n",
    "    pixels below a certain value -> black\n",
    "    pixels above a certain value -> white\n",
    "    '''\n",
    "    grayscale_img = image.convert('L')\n",
    "    img_array = np.array(grayscale_img)\n",
    "    height, width = img_array.shape\n",
    "\n",
    "    #an empty array for the output\n",
    "    thresholded_image = np.zeros((height, width))\n",
    "    \n",
    "    for y in range(0,height, block_size):\n",
    "        for x in range(0, width, block_size):\n",
    "            block = img_array[y:y+block_size, x:x+block_size]\n",
    "            #find the mean\n",
    "            block_mean = np.mean(block)\n",
    "            thresholded_block = (block > (block_mean-offset)).astype(np.uint8)*255\n",
    "            thresholded_image[y:y+block_size, x:x+block_size] = thresholded_block\n",
    "    return Image.fromarray(thresholded_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46594a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negate(image):\n",
    "    '''\n",
    "    White background, black text -> black background, white text\n",
    "    '''\n",
    "    img_arr = np.array(image)\n",
    "    height, width = img_arr.shape\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            img_arr[y,x] = (img_arr[y,x]==0).astype(np.uint8)*255\n",
    "    return Image.fromarray(img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "124a3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(image):\n",
    "    '''\n",
    "    convert the image to PIL image and show.\n",
    "    '''\n",
    "    if isinstance(image, np.ndarray):\n",
    "        Image.fromarray(image).show()\n",
    "    elif isinstance(image, Image.Image):\n",
    "        image.show()\n",
    "    else:\n",
    "        print(\"Can't show image.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597531b6",
   "metadata": {},
   "source": [
    "### Noise Reduction, black background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4425bc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_image = adaptive_thresholding(inp_img, 200, 90)\n",
    "out_image.show(title='adaptive_thresholding')\n",
    "out_image = negate(out_image)\n",
    "out_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5641172",
   "metadata": {},
   "source": [
    "### Obtain bounding box coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3136bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find contours and get bounding box for each contour\n",
    "cnts, _ = cv2.findContours(np.array(out_image,dtype=np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = [c for c in cnts if cv2.contourArea(c)>3]\n",
    "boundingBoxes = [cv2.boundingRect(c) for c in cnts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1a343829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_segmentation(image):\n",
    "    def segment(img, top, bottom):\n",
    "        height,width = img.shape\n",
    "        return Image.fromarray(img).crop((0,top, width, bottom))\n",
    "    vertical_projection = np.sum(image, axis=1, keepdims=True)\n",
    "    line_gaps = np.where(vertical_projection==0)[0]\n",
    "    lines = [0]\n",
    "    height, width = image.shape\n",
    "    for i,y in enumerate(line_gaps):\n",
    "        if i == 0: continue\n",
    "        if line_gaps[i]-line_gaps[i-1]==1: continue\n",
    "        lines.append(line_gaps[i])\n",
    "    return [segment(image, lines[i-1], lines[i]) for i in range(1,len(lines))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a2fe537d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.Image.Image image mode=F size=602x40>,\n",
       " <PIL.Image.Image image mode=F size=602x44>,\n",
       " <PIL.Image.Image image mode=F size=602x29>,\n",
       " <PIL.Image.Image image mode=F size=602x69>,\n",
       " <PIL.Image.Image image mode=F size=602x11>,\n",
       " <PIL.Image.Image image mode=F size=602x22>,\n",
       " <PIL.Image.Image image mode=F size=602x70>,\n",
       " <PIL.Image.Image image mode=F size=602x37>,\n",
       " <PIL.Image.Image image mode=F size=602x112>,\n",
       " <PIL.Image.Image image mode=F size=602x9>,\n",
       " <PIL.Image.Image image mode=F size=602x30>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_segmentation(np.array(out_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "14ec3816",
   "metadata": {},
   "outputs": [],
   "source": [
    "line = line_segmentation(np.array(out_image))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d4aa288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6f5f7b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_segmentation(line):\n",
    "    def segment(img,left,right):\n",
    "        height, width = img.shape\n",
    "        return Image.fromarray(img).crop((left,0, right, height))  \n",
    "    word_gaps = np.where(np.sum(line, axis=0)==0)[0]\n",
    "    words = [0]\n",
    "    for i,x in enumerate(word_gaps):\n",
    "        if i == 0: continue\n",
    "        if word_gaps[i]-word_gaps[i-1]>5: \n",
    "            show(segment(line, word_gaps[i-1], word_gaps[i]))\n",
    "            words.append(word_gaps[i])\n",
    "    \n",
    "    return words\n",
    "# print(word_gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "22af062d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 226, 242, 264, 276, 298, 310]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_segmentation(np.array(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5599b14a",
   "metadata": {},
   "source": [
    "#### Verify the boxes on the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "071336a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = cv2.drawContours(np.array(out_image, dtype=np.uint8),cnts, -1, (0,255,0), thickness=1, lineType=cv2.LINE_AA)\n",
    "show(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "60cda5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final=cv2.cvtColor(np.array(out_image, dtype=np.uint8), cv2.COLOR_RGB2BGR)\n",
    "for b in boundingBoxes:\n",
    "    x,y,w,h = b\n",
    "    final = cv2.rectangle(final, (x,y), (x+w,y+h), (0,255,0), 1)\n",
    "# Image.fromarray(final).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "06bb75d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dd275651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15, 96, 23, 11), (16, 445, 79, 25), (25, 299, 23, 13), (29, 373, 26, 20), (29, 407, 14, 18), (31, 155, 11, 17), (32, 257, 11, 18), (37, 445, 21, 18), (38, 101, 7, 7), (38, 199, 17, 11), (43, 259, 22, 18), (44, 157, 19, 16), (47, 58, 25, 16), (47, 410, 10, 17), (50, 97, 37, 11), (50, 307, 7, 7), (55, 186, 7, 22), (61, 375, 12, 13), (63, 199, 13, 11), (63, 303, 24, 9), (65, 411, 18, 12), (66, 159, 16, 14), (69, 263, 13, 13), (72, 414, 24, 20), (76, 362, 19, 26), (77, 198, 8, 17), (78, 59, 24, 11), (83, 261, 22, 25), (84, 300, 4, 7), (84, 392, 9, 11), (86, 157, 19, 20), (88, 200, 16, 11), (88, 446, 15, 15), (89, 301, 10, 12), (92, 96, 9, 12), (93, 408, 13, 17), (98, 57, 10, 27), (103, 302, 13, 11), (104, 99, 17, 8), (108, 447, 15, 13), (109, 200, 15, 9), (110, 409, 7, 14), (113, 58, 17, 15), (114, 288, 10, 26), (117, 59, 6, 7), (122, 409, 23, 18), (123, 96, 11, 13), (124, 263, 24, 12), (126, 372, 13, 14), (127, 161, 21, 12), (128, 198, 26, 11), (128, 303, 28, 13), (131, 60, 10, 19), (139, 371, 18, 15), (140, 95, 6, 8), (146, 56, 10, 15), (150, 97, 25, 9), (150, 268, 7, 7), (151, 166, 5, 7), (158, 196, 22, 13), (159, 263, 34, 12), (160, 445, 24, 18), (162, 56, 9, 14), (162, 160, 35, 13), (164, 301, 13, 15), (164, 361, 12, 24), (172, 89, 7, 6), (180, 301, 25, 21), (183, 197, 11, 18), (187, 410, 14, 14), (188, 445, 23, 16), (196, 264, 9, 14), (199, 198, 16, 12), (201, 160, 9, 12), (201, 376, 19, 10), (202, 464, 16, 9), (204, 411, 14, 14), (204, 434, 8, 9), (205, 56, 12, 14), (208, 267, 16, 11), (211, 302, 11, 16), (212, 91, 12, 16), (213, 161, 20, 10), (215, 12, 11, 17), (216, 450, 7, 9), (217, 360, 8, 26), (219, 411, 10, 16), (220, 203, 5, 7), (221, 59, 19, 10), (224, 304, 10, 17), (225, 93, 21, 13), (227, 373, 20, 14), (228, 10, 14, 17), (231, 266, 32, 11), (235, 412, 11, 13), (239, 160, 35, 11), (242, 306, 26, 11), (243, 56, 27, 13), (243, 357, 20, 28), (245, 400, 9, 26), (246, 16, 18, 12), (249, 94, 14, 12), (251, 14, 7, 8), (259, 361, 21, 23), (260, 410, 19, 14), (263, 254, 7, 27), (266, 93, 23, 20), (269, 14, 7, 14), (270, 58, 8, 18), (272, 265, 19, 14), (273, 144, 9, 28), (277, 33, 21, 7), (280, 14, 14, 18), (283, 411, 8, 12), (285, 160, 12, 11), (286, 97, 9, 7), (294, 265, 10, 18), (294, 372, 20, 26), (296, 411, 29, 11), (298, 299, 14, 14), (299, 56, 2, 11), (300, 147, 18, 23), (300, 173, 17, 9), (303, 19, 7, 11), (303, 371, 15, 17), (308, 269, 9, 8), (309, 58, 8, 11), (313, 300, 10, 14), (315, 105, 4, 5), (316, 400, 7, 7), (317, 44, 8, 23), (320, 374, 9, 14), (325, 300, 21, 15), (331, 57, 44, 10), (335, 373, 20, 14), (340, 258, 21, 19), (345, 158, 19, 11), (351, 302, 18, 14), (351, 359, 10, 25), (364, 263, 30, 15), (365, 406, 31, 19), (366, 154, 18, 15), (366, 373, 22, 11), (374, 301, 8, 15), (381, 363, 9, 8), (386, 56, 8, 9), (387, 156, 37, 13), (389, 305, 28, 9), (400, 263, 12, 15), (401, 56, 14, 10), (401, 412, 18, 9), (405, 266, 17, 16), (409, 294, 8, 5), (415, 395, 10, 27), (418, 55, 12, 12), (422, 251, 8, 23), (422, 373, 33, 16), (430, 155, 10, 12), (430, 410, 30, 16), (435, 263, 24, 13), (445, 157, 18, 9), (447, 300, 13, 15), (449, 403, 8, 6), (455, 52, 18, 12), (457, 250, 9, 25), (462, 373, 13, 13), (463, 299, 19, 14), (467, 260, 23, 15), (468, 162, 8, 6), (474, 54, 15, 11), (483, 362, 21, 23), (485, 300, 19, 14), (486, 411, 15, 16), (490, 388, 14, 11), (492, 260, 11, 18), (493, 53, 12, 10), (493, 64, 10, 8), (504, 410, 16, 17), (508, 265, 9, 8), (508, 469, 28, 2), (509, 300, 8, 14), (510, 52, 34, 11), (514, 299, 28, 23), (522, 280, 78, 2), (522, 411, 18, 14), (541, 306, 8, 6), (543, 411, 9, 13), (546, 55, 9, 14), (555, 410, 37, 23), (561, 51, 29, 16), (575, 312, 23, 6), (583, 417, 6, 8)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(boundingBoxes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d89c33e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_images = []\n",
    "for b in sorted(boundingBoxes, key=lambda x: x[1]):\n",
    "    x, y, w, h = b\n",
    "    character = np.array(out_image)[y:y+h, x:x+w]\n",
    "    character_images.append(character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7c0a5cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(character_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "737a0d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {}\n",
    "for letter in os.listdir('alphabet'):\n",
    "    png = Image.open('alphabet/'+letter) \n",
    "    background = Image.new('RGBA', png.size, (255,255,255))\n",
    "    background.paste(png, mask=png.split()[3])\n",
    "    background = background.convert('L')\n",
    "    templates[letter[-8:-4]] = negate(background)\n",
    "show(templates['3454'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b0c1fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_characters = []\n",
    "for ch in character_images:\n",
    "    best = -1\n",
    "    best_char = None\n",
    "    for char, template in templates.items():\n",
    "        template_np = np.array(template, dtype=np.uint8)\n",
    "        # ch = cv2.cvtColor(ch, cv2.COLOR_BGR2GRAY)\n",
    "        ch = cv2.resize(ch, template_np.shape, interpolation=cv2.INTER_AREA)\n",
    "        # show(ch)\n",
    "        # print(ch.shape, template_np.shape)\n",
    "        result = cv2.matchTemplate(np.array(ch, dtype=np.uint8), template_np, cv2.TM_CCOEFF_NORMED)\n",
    "\n",
    "        match_score =np.max(result)\n",
    "\n",
    "        if match_score > best:\n",
    "            best = match_score\n",
    "            best_match_char = char\n",
    "        # break\n",
    "    output_characters.append(best_match_char)\n",
    "    # print(best_match_char, end=\" \")\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "258e694c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3357', '3367', '3361', '3372', '3346', '3379', '3358', '3364', '3362', '3356', '3335', '3384', '3346', '3346', '3358', '3346', '3377', '3379', '3359', '3353', '3346', '3335', '3346', '3351', '3358', '3346', '3363', '3376', '3346', '3376', '3352', '3365', '3358', '3346', '3367', '3367', '.png', '3378', '3372', '3352', '3379', '3357', '3368', '3357', '3454', '3368', '3376', '3367', '3356', '3343', '3385', '3351', '3379', '3358', '3364', '3359', '3383', '3451', '3346', '3357', '3374', '3381', '3379', '3368', '3346', '3351', '3379', '3346', '3353', '3371', '3379', '3363', '3370', '3351', '3363', '3371', '3346', '3376', '3376', '3351', '3343', '3358', '3357', '3379', '3380', '3358', '3370', '3381', '3378', '3379', '3346', '3372', '3351', '3380', '3371', '3379', '3367', '3374', '3346', '3354', '3375', '3359', '3346', '3367', '3355', '3385', '3379', '3352', '3376', '3373', '3368', '3367', '3367', '3370', '3374', '3379', '3346', '3367', '3361', '3372', '3379', '3380', '3346', '3366', '3382', '3354', '3355', '3454', '3367', '3352', '3351', '3379', '3376', '3385', '3367', '3371', '3354', '3385', '3379', '3367', '3384', '3357', '3376', '3353', '3367', '3382', '3379', '3376', '3379', '3381', '3379', '3361', '3376', '3350', '3343', '3372', '3367', '3372', '3379', '3375', '3384', '3363', '3379', '3379', '3350', '3359', '3358', '3452', '3379', '3379', '3371', '3337', '3370', '3453', '3379', '3379', '3374', '3351', '3346', '3346', '3383', '3375', '3379', '3379', '3362', '3373', '3376', '3361', '3346', '3346', '3346', '3379']\n"
     ]
    }
   ],
   "source": [
    "print(output_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1f11553d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ഝ ധ ഡ ബ ഒ ള ഞ ത ഢ ജ ഇ സ ഒ ഒ ഞ ഒ റ ള ട ങ ഒ ഇ ഒ ഗ ഞ ഒ ണ ര ഒ ര ഗ ഥ ഞ ഒ ധ ധ .png\n",
      "ല ബ ഗ ള ഝ ന ഝ ൾ ന ര ധ ജ ഏ ഹ ഗ ള ഞ ത ട ഷ ൻ ഒ ഝ മ വ ള ന ഒ ഗ ള ഒ ങ ഫ ള ണ പ ഗ ണ ഫ ഒ ര ര ഗ ഏ ഞ ഝ ള ഴ ഞ പ വ ല ള ഒ ബ ഗ ഴ ഫ ള ധ മ ഒ ച യ ട ഒ ധ ഛ ഹ ള ഗ ര ഭ ന ധ ധ പ മ ള ഒ ധ ഡ ബ ള ഴ ഒ ദ ശ ച ഛ ൾ ധ ഗ ഗ ള ര ഹ ധ ഫ ച ഹ ള ധ സ ഝ ര ങ ധ ശ ള ര ള വ ള ഡ ര ഖ ഏ ബ ധ ബ ള യ സ ണ ള ള ഖ ട ഞ ർ ള ള ഫ ഉ പ ൽ ള ള മ ഗ ഒ ഒ ഷ യ ള ള ഢ ഭ ര ഡ ഒ ഒ ഒ ള "
     ]
    }
   ],
   "source": [
    "for enc in output_characters:\n",
    "    if enc not in encoding.encoding:\n",
    "        print(enc)\n",
    "        continue\n",
    "    print(encoding.encoding[enc], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d7d191b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(character_images[6])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
